# LangChain Restaurant Name Suggester - Complete Guide

A comprehensive journey through LangChain framework, starting with a simple restaurant name suggester and evolving into a multi-agent system with Wikipedia search, web search, and conversation memory.

## üìã Table of Contents

- [Introduction to LangChain](#introduction-to-langchain)
- [Project 1: Restaurant Name Suggester](#project-1-restaurant-name-suggester)
- [Project 2: Wikipedia Agent](#project-2-wikipedia-agent)
- [Project 3: SerpAPI Web Search Agent](#project-3-serpapi-web-search-agent)
- [Project 4: Memory Agent](#project-4-memory-agent)
- [Project 5: Combined Multi-Agent System](#project-5-combined-multi-agent-system)
- [Installation & Setup](#installation--setup)
- [Technical Deep Dive](#technical-deep-dive)

## üöÄ Introduction to LangChain

LangChain is a powerful framework for developing applications powered by language models. It provides:

- **LLM Integration**: Easy connection to various language models (OpenAI, Anthropic, etc.)
- **Prompt Management**: Templates and dynamic prompt construction
- **Memory Systems**: Conversation history and context management
- **Tool Integration**: Connect LLMs to external APIs and services
- **Agent Framework**: Build autonomous AI agents that can reason and act

### Core Concepts

```python
# Basic LangChain structure
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# Initialize LLM
llm = ChatOpenAI(temperature=0.7)

# Create prompt template
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")

# Chain them together
chain = prompt | llm

# Execute
result = chain.invoke({"topic": "artificial intelligence"})
```

### Project Architecture Evolution

```
üçΩÔ∏è Project 1: Restaurant Suggester
    ‚Üì
üìö Project 2: Wikipedia Agent  
    ‚Üì
üåê Project 3: SerpAPI Agent
    ‚Üì
üß† Project 4: Memory Agent
    ‚Üì
üîÑ Project 5: Combined System

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Simple Prompt + LLM ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ üçΩÔ∏è Restaurant      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   Suggester        ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM + External API  ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ üìö Wikipedia       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   Agent            ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM + Real-time     ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ üåê SerpAPI         ‚îÇ
‚îÇ Data                ‚îÇ    ‚îÇ   Agent            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM + Conversation  ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ üß† Memory          ‚îÇ
‚îÇ Memory              ‚îÇ    ‚îÇ   Agent            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Intelligent Agent   ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ üîÑ Combined        ‚îÇ
‚îÇ Router              ‚îÇ    ‚îÇ   System           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üçΩÔ∏è Project 1: Restaurant Name Suggester

Let's start with a simple restaurant name suggester to understand LangChain basics.

### Basic Implementation

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

# Initialize the LLM
llm = ChatOpenAI(
    temperature=0.7,  # Higher temperature for creative names
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# Create a prompt template
prompt_template = ChatPromptTemplate.from_template(
    """You are a creative restaurant naming expert. 
    Generate 5 unique and catchy restaurant names for a {cuisine_type} restaurant.
    The restaurant should have a {atmosphere} atmosphere.
    
    Provide names that are:
    - Memorable and easy to pronounce
    - Relevant to the cuisine type
    - Suitable for the atmosphere
    
    Cuisine: {cuisine_type}
    Atmosphere: {atmosphere}
    
    Restaurant Names:"""
)

def suggest_restaurant_names(cuisine_type, atmosphere):
    """Generate restaurant name suggestions"""
    # Create the chain
    chain = prompt_template | llm
    
    # Execute the chain
    response = chain.invoke({
        "cuisine_type": cuisine_type,
        "atmosphere": atmosphere
    })
    
    return response.content
```

### Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Input    ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Prompt Template ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ LangChain Chain ‚îÇ
‚îÇ Cuisine +       ‚îÇ    ‚îÇ {cuisine_type}  ‚îÇ    ‚îÇ  prompt | llm   ‚îÇ
‚îÇ Atmosphere      ‚îÇ    ‚îÇ {atmosphere}    ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Creative Names  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÇ   OpenAI GPT    ‚îÇ
‚îÇ    Output       ‚îÇ    ‚îÇ temperature=0.7 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Flow: Input ‚Üí Template ‚Üí Chain ‚Üí LLM ‚Üí Creative Output
```

### Key Learning Points

1. **Temperature Setting**: Higher values (0.7) for creativity, lower (0.1) for consistency
2. **Prompt Engineering**: Clear instructions lead to better outputs
3. **Template Variables**: Use `{variable}` for dynamic content
4. **Chain Composition**: `prompt | llm` creates a processing pipeline

## üìö Project 2: Wikipedia Agent

Now let's add external data sources using Wikipedia integration.

### Implementation

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

load_dotenv()

# Initialize components
llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())

def search_wikipedia(query: str) -> str:
    """Search Wikipedia and return summarized results"""
    try:
        result = wikipedia.run(query)
        # Limit response length for better processing
        return result[:500] + "..." if len(result) > 500 else result
    except Exception as e:
        return f"Error searching Wikipedia: {str(e)}"

def ask_with_wikipedia(question: str) -> str:
    """Answer questions using Wikipedia knowledge"""
    # Get Wikipedia information
    wiki_info = search_wikipedia(question)
    
    # Use LLM to provide a comprehensive answer
    prompt = f"""Based on this Wikipedia information: {wiki_info}
    
    Please provide a clear and informative answer to: {question}
    
    Make the answer concise but comprehensive."""
    
    response = llm.invoke(prompt)
    return response.content
```

### Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Question   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Wikipedia Search‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Wikipedia API   ‚îÇ
‚îÇWikipediaQueryRun‚îÇ    ‚îÇ   Raw Data      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                      ‚ñº
          ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ            ‚îÇResponse Process ‚îÇ
          ‚îÇ            ‚îÇTruncate 500char‚îÇ
          ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Error Handling  ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  LLM Synthesis  ‚îÇ
‚îÇ   Graceful      ‚îÇ    ‚îÇ   ChatOpenAI    ‚îÇ
‚îÇ   Fallback      ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇFormatted Answer ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Flow: Question ‚Üí Wikipedia ‚Üí Process ‚Üí LLM ‚Üí Answer
```

### Key Learning Points

1. **Tool Integration**: LangChain provides wrappers for external APIs
2. **Error Handling**: Always handle API failures gracefully
3. **Response Processing**: Truncate long responses for better LLM processing
4. **Information Synthesis**: LLM combines external data with reasoning

## üåê Project 3: SerpAPI Web Search Agent

Add real-time web search capabilities for current information.

### Implementation

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.utilities import SerpAPIWrapper

load_dotenv()

# Initialize components
llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))
serpapi = SerpAPIWrapper(serpapi_api_key=os.getenv("SERPAPI_API_KEY"))

def search_web(query: str) -> str:
    """Search the web using SerpAPI"""
    try:
        result = serpapi.run(query)
        return str(result)
    except Exception as e:
        return f"Error searching web: {str(e)}"

def ask_with_search(question: str) -> str:
    """Answer questions using web search"""
    # Search for current information
    search_result = search_web(question)
    
    # Use LLM to synthesize the information
    prompt = f"""Based on this web search result: {search_result}
    
    Please provide a clear and up-to-date answer to: {question}
    
    Focus on the most relevant and recent information."""
    
    response = llm.invoke(prompt)
    return response.content
```

### Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Query    ‚îÇ
‚îÇCurrent Info Need‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SerpAPI Search ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇWeb Search Result‚îÇ
‚îÇ SerpAPIWrapper  ‚îÇ    ‚îÇ Real-time Data  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñ≤                      ‚ñº
          ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇResult Processing‚îÇ
‚îÇ API Rate Limits ‚îÇ    ‚îÇRaw to Structured‚îÇ
‚îÇ Error Handling  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚ñº
          ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  LLM Analysis   ‚îÇ
                       ‚îÇ   ChatOpenAI    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ Current Answer  ‚îÇ
                       ‚îÇUp-to-date Info ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Flow: Query ‚Üí SerpAPI ‚Üí Process ‚Üí LLM ‚Üí Current Answer
```

### Key Learning Points

1. **Real-time Data**: Web search provides current information
2. **API Integration**: SerpAPI wrapper handles search complexity
3. **Data Synthesis**: LLM processes raw search results into answers
4. **Current Events**: Perfect for time-sensitive queries

## üß† Project 4: Memory Agent

Implement conversation memory for context-aware interactions.

### Implementation

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# Custom memory implementation
class SimpleMemory:
    def __init__(self, k=5):
        self.k = k  # Number of conversations to remember
        self.conversations = []
    
    def save_context(self, inputs, outputs):
        """Save conversation to memory"""
        self.conversations.append({
            "input": inputs["input"], 
            "output": outputs["output"]
        })
        # Keep only last k conversations
        if len(self.conversations) > self.k:
            self.conversations.pop(0)
    
    @property
    def buffer(self):
        """Get formatted conversation history"""
        return "\n".join([
            f"Human: {conv['input']}\nAI: {conv['output']}" 
            for conv in self.conversations
        ])
    
    def get_context(self):
        """Get conversation context for prompts"""
        return self.buffer

load_dotenv()

# Initialize components
llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))
memory = SimpleMemory(k=5)

def ask_with_memory(question: str) -> str:
    """Ask questions with conversation memory"""
    # Get conversation context
    context = memory.get_context()
    
    # Build prompt with context
    if context:
        prompt = f"""Previous conversation:
{context}

Human: {question}
AI:"""
    else:
        prompt = f"Human: {question}\nAI:"
    
    response = llm.invoke(prompt)
    answer = response.content
    
    # Save to memory
    memory.save_context({"input": question}, {"output": answer})
    return answer
```

### Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Input    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Memory Check   ‚îÇ
‚îÇ  get_context()  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇHas      ‚îÇ
     ‚îÇContext? ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      Yes ‚îÇ No
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇBuild      ‚îÇ         ‚îÇ Simple Prompt   ‚îÇ
    ‚îÇContextual ‚îÇ         ‚îÇ Current Only    ‚îÇ
    ‚îÇPrompt     ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ LLM Processing  ‚îÇ
          ‚îÇ   ChatOpenAI    ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇGenerate Response‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇSave to Memory   ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Memory Buffer   ‚îÇ
          ‚îÇsave_context()   ‚îÇ    ‚îÇSliding Window   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    (k=5)        ‚îÇ
                    ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº                      ‚ñ≤
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
          ‚îÇ Return Answer   ‚îÇ              ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
                                          ‚îÇ
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇMemory Management‚îÇ
                              ‚îÇ  Auto-cleanup   ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Flow: Input ‚Üí Memory Check ‚Üí Context Decision ‚Üí LLM ‚Üí Save ‚Üí Answer
```

### Key Learning Points

1. **Memory Management**: Custom implementation for conversation history
2. **Context Building**: Include previous conversations in prompts
3. **Sliding Window**: Keep only recent conversations to manage token limits
4. **State Management**: Maintain conversation state across interactions

## üîÑ Project 5: Combined Multi-Agent System

Bring everything together in a unified system that intelligently routes queries to the appropriate agent.

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Wikipedia     ‚îÇ    ‚îÇ    SerpAPI      ‚îÇ    ‚îÇ     Memory      ‚îÇ
‚îÇ     Agent       ‚îÇ    ‚îÇ     Agent       ‚îÇ    ‚îÇ     Agent       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Main Agent    ‚îÇ
                    ‚îÇ  (Router)       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation

```python
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper, SerpAPIWrapper

# Import our custom memory
class SimpleMemory:
    def __init__(self, k=3):
        self.k = k
        self.conversations = []
    
    def save_context(self, inputs, outputs):
        self.conversations.append({"input": inputs["input"], "output": outputs["output"]})
        if len(self.conversations) > self.k:
            self.conversations.pop(0)
    
    def get_context(self):
        return "\n".join([f"Human: {conv['input']}\nAI: {conv['output']}" for conv in self.conversations])

load_dotenv()

# Initialize all components
llm = ChatOpenAI(temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
serpapi = SerpAPIWrapper(serpapi_api_key=os.getenv("SERPAPI_API_KEY"))
memory = SimpleMemory(k=3)

def intelligent_agent(question: str) -> str:
    """Route questions to appropriate agent based on content"""
    question_lower = question.lower()
    
    # Route to Wikipedia for factual/historical queries
    if any(word in question_lower for word in ['what is', 'who was', 'tell me about', 'explain']):
        wiki_result = wikipedia.run(question)[:500]
        prompt = f"Based on Wikipedia: {wiki_result}\n\nAnswer: {question}"
        response = llm.invoke(prompt)
        answer = response.content
    
    # Route to SerpAPI for current information
    elif any(word in question_lower for word in ['current', 'latest', 'today', 'weather', 'news']):
        search_result = serpapi.run(question)
        prompt = f"Based on search: {search_result}\n\nAnswer: {question}"
        response = llm.invoke(prompt)
        answer = response.content
    
    # Use memory for conversational queries
    else:
        context = memory.get_context()
        if context:
            prompt = f"Previous conversation:\n{context}\n\nHuman: {question}\nAI:"
        else:
            prompt = f"Human: {question}\nAI:"
        response = llm.invoke(prompt)
        answer = response.content
    
    # Save to memory
    memory.save_context({"input": question}, {"output": answer})
    return answer
```

### Interactive Architecture Diagram

```mermaid
flowchart TD
    A["User Question"] --> B["Query Analysis<br/>intelligent_agent()"]
    B --> C{"Query Type?"}
    
    C -->|"what is, who was, explain"| D["üìö Wikipedia Route"]
    C -->|"current, latest, weather"| E["üåê SerpAPI Route"]
    C -->|"conversational"| F["üß† Memory Route"]
    
    D --> G["Wikipedia Search<br/>WikipediaQueryRun"]
    E --> H["Web Search<br/>SerpAPIWrapper"]
    F --> I["Context Retrieval<br/>SimpleMemory"]
    
    G --> J["Wikipedia Data"]
    H --> K["Real-time Data"]
    I --> L["Conversation Context"]
    
    J --> M["LLM Processing<br/>ChatOpenAI"]
    K --> M
    L --> M
    
    M --> N["Generated Response"]
    N --> O["Save to Memory<br/>All Routes"]
    O --> P["Final Answer"]
    
    Q["Error Handling<br/>Graceful Fallback"] --> D
    Q --> E
    Q --> F
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e1f5fe
    style E fill:#e8f5e8
    style F fill:#fce4ec
    style G fill:#f1f8e9
    style H fill:#f1f8e9
    style I fill:#f1f8e9
    style M fill:#ffecb3
    style O fill:#c8e6c9
    style P fill:#a5d6a7
    style Q fill:#ffcdd2
```

### Key Features

1. **Intelligent Routing**: Automatically selects the best agent for each query
2. **Unified Interface**: Single function handles all agent types
3. **Memory Integration**: All interactions are remembered
4. **Fallback Handling**: Graceful degradation when APIs fail

## üöÄ Installation & Setup

### Prerequisites

- Python 3.8 or higher
- pip package manager
- OpenAI API key
- SerpAPI key (optional, for web search)

### Required Packages

```bash
pip install langchain==0.3.26
pip install langchain-community==0.3.26
pip install langchain-core==0.3.74
pip install langchain-openai==0.3.24
pip install python-dotenv==1.0.0
pip install wikipedia==1.4.0
pip install google-search-results==2.4.2
```

### Quick Installation

```bash
# Install all dependencies
pip install -r requirements.txt

# Create environment file
copy .env.example .env
# Edit .env with your API keys
```

### Environment Setup

Create a `.env` file:

```env
# Required
OPENAI_API_KEY=your_openai_api_key_here

# Optional (for web search)
SERPAPI_API_KEY=your_serpapi_key_here
```

### Running the Projects

```bash
# Project 1: Restaurant Name Suggester
python restaurant_suggester.py

# Project 2: Wikipedia Agent
python wikipedia_agent.py

# Project 3: SerpAPI Agent
python serpapi_agent.py

# Project 4: Memory Agent
python memory_agent.py

# Project 5: Combined System
python agent.py

# Interactive Demo
python interactive_demo.py
```

## üîß Technical Deep Dive

### LangChain Framework Architecture

**Core Components**:
- **LLMs**: Language model integration (`ChatOpenAI`)
- **Prompts**: Template-based prompt construction
- **Tools**: External API wrappers (Wikipedia, SerpAPI)
- **Memory**: Conversation state management
- **Chains**: Component composition and execution

### Complete System Architecture

```mermaid
graph TB
    subgraph "üçΩÔ∏è Project 1: Restaurant Suggester"
        A1["Prompt Template"] --> A2["LLM"] --> A3["Creative Names"]
    end
    
    subgraph "üìö Project 2: Wikipedia Agent"
        B1["Query"] --> B2["Wikipedia API"] --> B3["LLM Synthesis"] --> B4["Factual Answer"]
    end
    
    subgraph "üåê Project 3: SerpAPI Agent"
        C1["Search Query"] --> C2["SerpAPI"] --> C3["LLM Processing"] --> C4["Current Info"]
    end
    
    subgraph "üß† Project 4: Memory Agent"
        D1["User Input"] --> D2["Memory Context"] --> D3["LLM + History"] --> D4["Contextual Response"]
        D4 --> D5["Update Memory"]
        D5 --> D2
    end
    
    subgraph "üîÑ Project 5: Combined System"
        E1["User Query"] --> E2["Query Router"]
        E2 --> E3["Wikipedia Agent"]
        E2 --> E4["SerpAPI Agent"]
        E2 --> E5["Memory Agent"]
        E3 --> E6["Unified Memory"]
        E4 --> E6
        E5 --> E6
        E6 --> E7["Final Response"]
    end
    
    style A1 fill:#e1f5fe
    style B2 fill:#f3e5f5
    style C2 fill:#e8f5e8
    style D2 fill:#fff3e0
    style E2 fill:#fce4ec
    style E6 fill:#f1f8e9
```

### Evolution of Complexity

1. **Basic Chain**: `Prompt | LLM`
2. **Tool Integration**: `LLM + External APIs`
3. **Memory Addition**: `Context + LLM + Memory Storage`
4. **Agent Routing**: `Query Analysis + Tool Selection + Response`

### Memory System Design

```mermaid
sequenceDiagram
    participant U as User
    participant M as Memory System
    participant L as LLM
    participant S as Storage
    
    U->>M: User Input
    M->>S: get_context()
    S-->>M: Previous Conversations
    M->>L: Contextual Prompt
    L-->>M: Generated Response
    M->>S: save_context(input, output)
    S->>S: Sliding Window (k=5)
    M-->>U: Final Answer
    
    Note over S: Memory Structure<br/>{"conversations": [<br/>{"input": "question", "output": "answer"}<br/>]}
```

```python
# Memory Flow
User Input ‚Üí Context Retrieval ‚Üí Prompt Construction ‚Üí LLM Processing ‚Üí Response + Memory Update

# Memory Structure
{
    "conversations": [
        {"input": "user_question", "output": "ai_response"},
        {"input": "follow_up", "output": "contextual_response"}
    ]
}
```

### Performance Considerations

- **Token Management**: Truncate long responses to stay within limits
- **Memory Efficiency**: Sliding window approach for conversation history
- **API Rate Limits**: Built-in retry mechanisms and error handling
- **Response Caching**: LangChain automatic caching for repeated queries

### Best Practices Learned

1. **Prompt Engineering**: Clear, specific instructions yield better results
2. **Error Handling**: Always handle API failures gracefully
3. **Memory Management**: Balance context richness with token efficiency
4. **Tool Selection**: Route queries to the most appropriate data source
5. **User Experience**: Provide feedback during long-running operations

## üêõ Troubleshooting

### Common Issues

#### 1. Import Errors
```bash
# Error: ModuleNotFoundError: No module named 'langchain.memory'
# Solution: Use custom SimpleMemory class (already implemented)
```

#### 2. API Key Issues
```bash
# Error: OpenAI API key not found
# Solution: Check .env file and ensure OPENAI_API_KEY is set
```

#### 3. SerpAPI Errors
```bash
# Error: SerpAPI key invalid
# Solution: Verify SERPAPI_API_KEY in .env file
```

#### 4. Unicode Encoding
```python
# Handled automatically in code:
try:
    print(response)
except UnicodeEncodeError:
    safe_response = response.encode('ascii', 'ignore').decode('ascii')
    print(safe_response)
```

### Version Compatibility

| Component | Version | Notes |
|-----------|---------|-------|
| LangChain | 0.3.26+ | Core framework |
| Python | 3.8+ | Required for LangChain |
| OpenAI | Latest | API compatibility |

## üìà Next Steps

- **Database Integration**: Persistent memory storage
- **Multi-modal Support**: Image and audio processing
- **Custom Tools**: Domain-specific integrations
- **Performance Monitoring**: Response time tracking
- **Production Deployment**: Scaling and monitoring

---

**Note**: This project demonstrates the progressive complexity of LangChain applications, from simple prompt chains to sophisticated multi-agent systems. Each project builds upon the previous one, showing how to incrementally add capabilities while maintaining clean, maintainable code. Error: SerpAPI key invalid
# Solution: Verify SERPAPI_API_KEY in .env file
```

#### 4. Unicode Encoding
```python
# Handled automatically in code:
try:
    print(response)
except UnicodeEncodeError:
    safe_response = response.encode('ascii', 'ignore').decode('ascii')
    print(safe_response)
```

### Debug Mode

Enable verbose logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# For LangChain specific debugging
os.environ["LANGCHAIN_VERBOSE"] = "true"
```

### Version Compatibility

| Component | Version | Notes |
|-----------|---------|-------|
| LangChain | 0.3.26+ | Core framework |
| Python | 3.8+ | Required for LangChain |
| OpenAI | Latest | API compatibility |

## üìà Future Enhancements

- **Database Integration**: Persistent memory storage
- **Multi-modal Support**: Image and audio processing
- **Agent Orchestration**: Automatic agent selection
- **Performance Monitoring**: Response time tracking
- **Custom Tools**: Domain-specific integrations

## üìÑ License

This project is for educational and demonstration purposes. Please ensure compliance with API provider terms of service.

---

**Note**: Remember to keep your API keys secure and never commit them to version control. Use environment variables or secure key management systems in production environments.